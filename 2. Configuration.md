# 2. Configuration
- ConfigMap : 키-값 쌍으로 데이터를 저장하는데 사용하는 API 오브젝트로, 파드는 볼륨에서 환경 변수, 커맨드-라인 인수 또는 구성 파일로 컨피그맵을 사용할 수 있음.
  - ConfigMap 생성 예시(kubectl 명령어)
  ```
  kubectl create configmap <config-name> --from-literal=<key>=<value> # key-value 값 직접 지정하여 생성  
  kubectl create configmap <config-name> --from-file=<path-to-file>   # 파일을 직접 지정하여 생성  
  kubectl create configmap <config-name> --from-file=<directory>/     # 디렉토리를 직접 지정하여 생성(디렉토리 내 파일은 key, 파일 내용은 value가 됨)  
  ```
    
  - yml 파일을 활용한 ConfigMap 생성 및 적용 예시
  ``` yaml
  # config-map.yml 파일 정의
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: app-config
  data:
    player_initial_lives: "3"
    ui_properties_file_name: "user-interface.properties"
  ```  
  ```  yaml
  # pod-definition.yml
  apiVersion: v1
  kind: Pod
  metadata:
    name: my-pod
    labels:
      app: myapp
  spec:
    containers
    - name: nginx-container
      image: nginx
  envFrom:
    - configMapRef: # ConfigMap 적용 
      name: app-config
  ```

- Secret : 컨테이너에서 사용하는 password, auth token, ssh key와 같은 중요 민감 정보들을 저장하고 base64로 인코딩해서 관리.  
  민감하지 않은 일반 설정 데이터는 ConfigMap을 사용하고, 민감한 정보는 Scret 사용.  
  - Secret 생성 예시(kubectl 명령어)  
  ```
  kubectl create secret generic <secret-name> --from-literal=<key>=<value>
  kubectl create secret generic <secret-name> --from-file=<path-to-file>
  ```
  - yml 파일을 활용한 Secret 생성 및 적용 예시
  ``` yaml
  # secret.yml
  apiVersion: v1
  kind: Secret
  metadata:
    name: mysecret
  data:  # 이 값은 Secret 생성 후 base64로 인코딩되어 관리됨
    USER_NAME: root
    PASSWORD: pswrd
  ```  
  ``` yaml
  # pod-definition.yml
  apiVersion: v1
  kind: Pod
  metadata:
    name: my-pod
    labels:
      app: myapp
  spec:
    containers:
    - name: nginx-container
      image: nginx
      env:
        - name: SECRET_USERNAME
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: USER_NAME
  ```
  
- Security Context : 파드/컨테이너 단계에서의 보안 설정 기능  
  ``` yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: security-context-demo
  spec:
    securityContext: # 파드 단계에서의 보안 설정
      runAsUser: 2000  # 파드를 2000 계정 권으로 실행
    containers:
    - name: busybox
      image: busybox:1.28
      securityContext:  # 컨테이너 단계에서의 보안 설정
        runAsUser: 3000  # 컨테이너를 3000 계정 권한으로 실행
  ```  
  - SecurityContext 옵션  
    1) `runAsUser` : 파드 또는 컨테이너를 실행시킬 PID를 지정  
       이때, 만약 runAsNonRoot 가 True 이고, runAsUser 값이 0(root)일 경우에는 파드 생성 불가.  
    3) `runAsGroup` : 파드 또는 컨테이너를 실행시킬 GID를 지정  
    4) `fsGroup` : 볼륨 마운트 시 활용할 PID를 지정  
    5) `runAsNonRoot` : 컨테이너를 루트가 아닌 사용자로 실행할지 지정  
  
  
- Service Account : 파드의 일부 컨테이너에서 실행되는 애플리케이션 프로세스(젠킨스, 프로메테우스 등)이  
  클러스터의 API 서버에 인증하기 위해 사용.  
  - Service Account 생성 예시  
   `kubectl create serviceaccount <sa-name>`  
  쿠버네티스는 파드에 Service Account를 명시하지 않을 경우 기본적으로 `default` Service Account와 그 토큰이 볼륨으로 마운트 됨.  
  쿠버네티스 v1.32을 포함한 최신 버전에서는  API 자격 증명들은 TokenRequest API를 사용하여 직접 얻거나  
  Projected Volume을 사용하여 파드에 마운트할 수 있음.  
  (단, 이 방법으로 취득한 토큰은 시간 제한이 있으며, 마운트 되었던 파드가 삭제되는 경우 자동으로 만료됨.)   
  만약 만료되지 않는 토큰이 필요한 경우, 다음과 같이 해당 서비스어카운트를 참조하는 어노테이션을 갖는  
 `kubernetes.io/service-account-token` 타입의 시크릿을 생성.  
  ``` yaml
  apiVersion: v1
  kind: Secret
  type: kubernetes.io/service-account-token
  metadata:
    name: mysecretname
    annotations:
      kubernetes.io/service-account.name: myserviceaccount # 서비스 계정과 연결
  ```  
- Resource Requirements : 파드가 사용하는 노드의 리소스 양을 제약 및 관리.
  - Pod별 리소스 관리  
    ``` yaml
    # pod-definition.yml
    apiVersion: v1
    kind: Pod
    metadata:
      name: my-pod
      labels:
        app: myapp
    spec:
      containers:  # 리스트 형식
      - name: nginx-container
        image: nginx
        resources:
          requests:
            memory: "1Gi"
            cpu: 1
          limits:
            memory: "2Gi"
            cpu: 1
    ```
  - `LimitRange`를 사용한 네임스페이스 단위 리소스 관리  
    네임스페이스 내 각각의 파드가 사용할 수 있는 리소스량의 한도 설정  
    ``` yaml
    apiVersion: v1
    kind: LimitRange
    metadata:
      name: my-resource-constraint
      namespace: test
    spec:
      limits:
      - max:
          cpu: "1"  # test 네임스페이스 내 각 파드는 cpu를 1 이상 점유 불가  
        type: Container
    ```  
  - `Resource Quota`를 사용한 네임스페이스 단위 리소스 총량 관리  
    네임스페이스 내 총 리소스 사용을 한도 설정  
    ``` yaml
    # resource-quota.yml
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: my-resource-quota
      namespace: test
    spec:
      hard:
        requests.cpu: 1
        requests.memory: 1Gi
        limits.cpu: 2  # test 네임스페이스 내 모든 파드가 점유하는 cpu 합은 2를 넘을 수 없음  
        limits.memory: 2Gi
        requests.nvidia.com/gpu: 4
    ```

- Taints & Tolerations : 워커 노드에 Taint가 설정된 경우, 동일 값의 Toleration이 있는 파드만 해당 워커 노드에 배치 가능.  
  Toleration이 있는 파드는 동일한 Taint가 있는 노드를 포함하여 모든 노드에 배치 가능.  
  가령, `kubectl create deployment testdep --image=nginx --replicas=5` 명령어를 통해 파드를 생성한 경우,  
  오직 워커노드에만 배치되고 절대 마스터 노드에는 배치되지 않는데,  
  그 이유는 마스터 노드에는 `node-role.kubernetes.io/master:NoSchedule` 라는 Taint가 설정되어 있기 때문.  
  - Taints 생성 예시  
    `kubectl taint nodes node1 key1=value1:NoSchedule`  
  - Taints 제거 예시  
    `kubectl taint nodes node1 key1=value1:NoSchedule-`
  - Tolerations 적용 예시
    ``` yaml
    # pod-definition.yml
    apiVersion: v1 
    kind: Pod
    metadata:
      name: my-pod
      labels:
        app: myapp
    spec:
      containers: 
      - name: nginx-container
        image: nginx
      tolerations: # tolerations 적용
      - key: "key1"
        operator: "Equal" # Taint가 '일치'하는 노드에 배치 가능
        value: "value1"
        effect: "NoSchedule"
    ```

  - effect 종류
  1) NoSchedule : toleration이 맞지 않으면 배치되지 않음.  
  2) PreferNoSchedule : toleration이 맞지 않으면 배치되지 않으나, 클러스터 리소스가 부족하면 배치됨.  
  3) NoExecute : toleration이 맞지 않으면 동작중인 파드를 종료시키고 해당 Taint 노드에 스케줄시키지 않음.  

- Node Selector & Node Affinity  
  - Node Selector : 파드가 특정 노드에만 할당되도록 설정  
  `kubectl label nodes <node-name> color=blue # 노드의 레이블 생성`  
  `kubectl get nodes --show-labels # 노드의 레이블 확인`  
    ``` yaml
    # pod-definition.yml
    apiVersion: v1 
    kind: Pod
    metadata:
      name: my-pod
      labels:
        app: myapp
    spec:
      containers: 
      - name: nginx-container
        image: nginx
      nodeSelector:
        color: blue # 노드의 레이블을 통해 선택
    ```
  => 단, 이러한 Node Selector는 오로지 명시한 레이블이 있는 노드만 선택이 가능하다는 제약이 있으며,  
     특정 파드가 여러 노드들 중 어느 하나라도 배치될 수 있게끔 하거나,  
     혹은 특정 노드를 제외하고 다른 노드에는 모두 배치 가능하게끔 하는 등 복잡한 노드 배치 조건으로써는 부족함.  
    
  - Node Affinity : `nodeSelector`처럼 노드의 레이블을 기반으로 파드가 스케줄링될 수 있는 노드를 제한할 수 있지만,  
    `nodeSelector`보다 다양한 제약 조건 사용 가능.  
    ``` yaml
    # pod-definition.yml
    apiVersion: v1 
    kind: Pod
    metadata:
      name: my-pod
      labels:
        app: myapp
    spec:
      containers: 
      - name: nginx-container
        image: nginx
      affinity:  
        nodeAffinity:  # Node Affinity 적용
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In  # NotIn, Exists 조건도 있음.
                values: # blue 또는 red 중에서는 어느 것에 배치되어도 상관 없음
                - blue
                - red
    ```
    - Node Affinity 종류  
    1) `requiredDuringSchedulingIgnoredDuringExecution` : 조건이 만족되지 않으면 스케줄러는 파드를 아예 배치하지 않음.  
    2) `preferredDuringSchedulingIgnoredDuringExecution` : 스케줄러가 최대한 조건에 맞는 노드를 찾되,  
       해당되는 노드가 없으면 파드를 아무 노드에 배치해도 됨.  
    => 단, 두 종류 모두 파드가 이미 실행중인 경우엔 Node Affinity가 중간에 바뀌어도 변화가 적용되지 않음.(`IgnoredDuringExecution`)  
